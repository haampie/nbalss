\documentclass[a4paper]{article}

\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{color}
\usepackage{pgfplots}
\usepackage[parfill]{parskip}
\usepackage{subcaption}
\usepackage[colorlinks = true, urlcolor=red]{hyperref}
\usepackage[font={small,it}]{caption}
\usepackage{booktabs}
\usepackage{cleveref}
\usepackage{autonum}

\definecolor{light-gray}{gray}{0.7}
\pgfplotsset{compat=newest}
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sspan}{span}
\DeclareMathOperator{\Ima}{Im}

\lstdefinestyle{myCustomMatlabStyle}{
  language=Matlab,
  tabsize=4,
  showspaces=false,
  showstringspaces=false
}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true,style=myCustomMatlabStyle}
\lstset{frame=tblr,rulecolor=\color{light-gray}}

\author{H.T. Stoppels (University of Groningen)}
\title{{\sc nbalss} Homework 8}

\begin{document}
  \maketitle 

  \paragraph{1} We're considering the LU and Cholesky decomposition of the tridiagonal $n\times n$ matrix
  \begin{equation}
    B(\mu) = \diag \begin{bmatrix}
      \frac{1}{h^2} & (\frac{-2}{h^2} + \mu) & \frac{1}{h^2}
    \end{bmatrix}
  \end{equation}
  where $h = \frac{1}{n + 1}$ and $\mu \in \{0, 5, 10, 20\}.$ Clearly, $B(\mu)$ is weakly diagonally dominant for $\mu = 0$ only. It is well-known that this property is maintained during the LU decomposition, so that pivoting will not be necessary. However, for $\mu > 0$ this is not the case, and at some step $k$ of the elimination process it may be that $|b^{(k)}_{k,k}| < |b^{(k)}_{k+1,k}|,$ so that the rows must be interchanged to guarantee numerical stability. Note that only two consecutive rows can be permuted because our matrix is tri-diagonal.

  In Matlab we can run {\tt [L, U, P] = lu(B)} to get the LU factorization (without pivoting) of $PB.$ It turns out that the LU decomposition starts without pivotting, but after a certain step $k$ it keeps on pivoting until the end. In our case of a tri-diagonal matrix this is the same as interchanging row $k$ and $n.$

  If we use {\tt [L, U] = lu(B)} however, Matlab gives the factorization $$\tilde{L}U = (P^TL)U = B$$ and typically $\tilde{L}$ will not be lower triangular because its rows are interchanged.

  \centerline{\rule{0.1\textwidth}{.4pt}}

  We can also try and compute the Cholesky decomposition of $-B(\mu).$ The minus here is necessary because the Cholesky decomposition applies only to hermetian, positive definite matrices, and $-B(\mu)$ satisfies these conditions clearly as long as $\mu < \lambda_1$ where $\lambda_1$ is the smallest eigenvalue of $-B(0),$ since
  \begin{equation}
     -B(0)x = \lambda x \iff -B(0)x - \mu x = (\lambda - \mu) x \iff -B(\mu)x = (\lambda - \mu) x.
  \end{equation}
  If $n$ is large enough this means that $\mu < \pi^2$ for $-B(\mu)$ to be positive definite.

  The conjugate gradient method is based on the idea that positive definite matrices induce an inner product, so for CG to work properly we would have the same condition on $\mu.$
  \paragraph{2} We now try to solve the indefinite system $B(\mu)x = b$ where $\mu > \pi^2.$ It seems that both GMRES and SYMMLQ diverge, even with an incomplete LU decomposition as preconditioner.

  So to solve the problem, we use a Galerkin projection into the space spanned by the eigenvectors corresponding to positive eigenvalues. The idea is that in this subspace the coefficient matrix will be positive definite, such that efficient methods can be applied.

  To be more precise, suppose $P$ is an $n \times m$ orthonormal\footnote{In our code $P$ is just orthogonal s.t. $P^TP = c I,$ but this just scales the Ritz values while keeping the Ritz vectors intact.} matrix with $m \ll n$ such that we can cheaply solve the Galerkin problem
  \begin{equation}
    Bv - \theta v \perp \Ima P \text{ with } v = Py \in \Ima P \text{ for } y \in \mathbb{R}^m.
  \end{equation}
  That is equivalent to solving the $m \times m$ eigenvalue problem
  \begin{equation}\label{eq:problem}
    P^TBPy = \theta y.
  \end{equation}
  We take $v = Py / \norm{Py}_2$ where $y$ is the eigenvector corresponding to the smallest eigenvalue of $P^TBP$ as an approximation to the ``bad'' eigenvector of $B.$

  Clearly $P_1 := I - vv^T$ is the natural projection on $\mathbb{R}^n \setminus \sspan v$ and $P_1$ is hermetian. Now we apply this projection to solve the problem $Bx = b$ in the Galerkin sense:
  \begin{equation}
    Bx^\perp - b \perp \Ima P_1 \text{ with } x^\perp = P_1y \text{ for } y \in \mathbb{R}^n
  \end{equation}
  which is equivalent to solving
  \begin{equation}\label{eq:proj_ort}
    P_1BP_1y = P_1b.
  \end{equation}
  Clearly $P_1BP_1$ is singular as a linear map from $\mathbb{R}^n$ onto itself, but it is invertible as a linear map from $\mathbb{R}^n \setminus \sspan v$ onto itself. In fact Krylov subspace methods don't bother since $$\mathcal{K}_\ell\left(P_1BP_1, P_1b\right) \subset \mathbb{R}^n$$ is orthogonal to $v$ by construction. So we write
  \begin{equation}
    y = (P_1BP_1)^{-1}P_1b \quad \Longrightarrow \quad x^\perp = P_1y = P_1(P_1BP_1)^{-1}P_1Bx
  \end{equation}
  and recognize $P_1(P_1BP_1)^{-1}P_1B$ as another projection with complementary projection $$P_2 := I - P_1(P_1BP_1)^{-1}P_1B$$
  which can be used to find $x^\| \in \Ima v$ via a Galerking projection:
  \begin{equation}
    Bx^\| - b \perp \Ima P_2 \text{ with } x^\| = P_2z \text{ for } z \in \mathbb{R}^n.
  \end{equation}
  which reduces to solving
  \begin{equation}\label{eq:proj_par}
    P_2^TBP_2z = P_2^Tb.
  \end{equation}

  \centerline{\rule{0.1\textwidth}{.4pt}}

  So all in all we must first solve Equation~\eqref{eq:proj_ort} and then Equation~\eqref{eq:proj_par} and assemble the solution as $x = x^\perp + x^\|.$ To do so we need to implement an efficient way to do MV products with $P_1$ and $P_2.$ For $P_1$ we simply have $P_1q = q - (v^Tq) v,$ which is an inner product and an AXPY operation. For $P_2$ we must multiply with $B$ and then $P_1$ as just described, solve a system involving $P_1BP_1,$ multiply again with $P_1$ and subtract two vectors. We also need the transpose $P_2^T = I - BP_1(P_1BP_1)^{-1}P_1$ which has the same elements in a different order.

  \newpage
  Finally we get the Matlab script
\begin{lstlisting}
function testdeflat()
  n = 100; m = 10; mu = 10;
  [B, rhs] = problem(n, mu);
  [L, U] = preconditioner(B);
  v = approximate_bad_eigenvector(B, m);

  % Projects z on R^n \ span{v}
  P1 = @(z) z - dot(v, z) * v;

  % Restrict B to R^n \ span{v}
  B1 = @(z) P1(B * P1(z));

  % Solves P1 B P1 x = b for x.
  solver = @(b) pcg(B1, b, 1e-12, 400, L, U);

  % Other projection and its transpose
  P2 = @(z) z - P1(solver(P1(B * z)));
  P2_t = @(z) z - B * P1(solver(P1(z)));

  % Restrict -B to image of P2
  B2 = @(z) -P2_t(B * P2(z));
  
  [x1, ~, ~, ~, history_1] = solver(P1(rhs));
  [x2, ~, ~, ~, history_2] = pcg(B2, -P2_t(rhs), 1e-12, 400);

  x = P1(x1) + P2(x2); 
  
  fprintf('|Residual| = %e\n', norm(B * x - rhs));
  semilogy(history_1); hold on; semilogy(history_2);
end

function [L, U] = preconditioner(B)
  opts.type = 'ilutp';
  opts.droptol = 1.0;
  [L, U] = ilu(B, opts); % Diagonal matrix
end

function [B, rhs] = problem(n, mu)
  B = -linbuck(n) - mu * speye(n);  
  rhs = sin(0.01 * (1 : n)'); % An arbitrary smooth rhs.
end

function v = approximate_bad_eigenvector(B, m)
  n = size(B, 1);
  P = kron(eye(m), ones(n / m, 1));
  [V, ~] = eig(P' * B * P);
  v = P * V(:, 1);
  v = v / norm(v);
end

function A = linbuck(n)
  e = ones(n, 1) * (n + 1) ^ 2;
  A = spdiags([e, -2 * e, e], -1 : 1, n, n);
end
\end{lstlisting}
  It turns out that the solving for $x^\perp$ requires about 100 iterations of CG, and solving for $x^\|$ is finished in a single iteration of CG. The normed residual $\norm{Bx - b}_2 \approx 6.80\cdot10^{-10},$ so the idea works. Storage is reduced to a set of vectors.
\end{document}